---
title: "Flights EDA"
output: html_notebook
---

```{r setup}
# Packages ----
library(sparklyr)
library(tidyverse)
library(nycflights13)

# Plot defaults ----
theme_set(theme_bw())
```

```{r spark-connection}
spark_home <- system("databricks-connect get-spark-home", intern = TRUE)
sc <- spark_connect(method = "databricks", spark_home = spark_home)
```

## Data
Airlines data is a public Databricks dataset stored in DBFS. Airline codes are scraped from
[Wikipedia](https://en.wikipedia.org/wiki/List_of_airline_codes) and then loaded
into Spark.
```{r}
if (!("all_flights" %in% odbc::dbListTables(sc))) {
  spark_read_csv(sc, 
                 name = "all_flights_raw", 
                 path = "/databricks-datasets/asa/airlines") %>% 
    mutate(
      DepTime = as.integer(DepTime),
      ArrTime = as.integer(ArrTime),
      TailNum = as.integer(TailNum),
      ActualElapsedTime = as.integer(ActualElapsedTime),
      CRSElapsedTime = as.integer(CRSElapsedTime),
      AirTime = as.integer(AirTime),
      ArrDelay = as.integer(ArrDelay),
      DepDelay = as.integer(DepDelay),
      Distance = as.integer(Distance),
      TaxiIn = as.integer(TaxiIn),
      TaxiOut = as.integer(TaxiOut),
      CarrierDelay = as.integer(CarrierDelay),
      WeatherDelay = as.integer(WeatherDelay),
      NASDelay = as.integer(NASDelay),
      SecurityDelay = as.integer(SecurityDelay),
      LateAircraftDelay = as.integer(LateAircraftDelay)
    ) %>% 
    left_join(tbl(sc, "airline_codes"), by = c("UniqueCarrier" = "iata")) %>% 
    filter(!is.na(airline)) %>% 
    spark_write_table(name = "all_flights", 
                      mode = "overwrite", 
                      partition_by = c("airline", "Year"),
                      options = list(path = "dbfs:/flights"))
}
```


Refer to the persistent table and cache it in memory
```{r}
tbl_cache(sc, "all_flights")
all_flights <- tbl(sc, "all_flights")
```

```{r}
class(all_flights)
```

## Exploration
How many rows of data are included in `all_flights`
```{r}
(n_flights <- tally(all_flights) %>% 
   collect())
```

What columns are included in `all_flights`
```{r}
head(all_flights)
```

How many records exist for each year?
```{r}
(year_counts <- count(all_flights, Year) %>% 
   arrange(Year) %>% 
   collect())
```

```{r}
year_counts %>% 
  ggplot(aes(x = Year, y = n)) +
  geom_col() +
  labs(title = "Flight Records per Year",
       x = "Year",
       y = "Flights")
```

How many airlines are represented in the data?
```{r}
all_flights %>% 
  count(airline) %>% 
  arrange(desc(n))
```

Visualization is a powerful tool for exploring data. However, in this case we
have `r n_flights` records, so pulling all of the data into R for visualization
isn't ideal. Instead, we can use existing tools to compute summarizations of the
data using Spark and then use ggplot2 to visualize those summarizations.

Distribution of departure delays by airline:
```{r}
all_flights %>% 
  dbplot:::dbplot_boxplot(airline, DepDelay) +
  coord_flip()
```


Unique flights defined by origin and destination
```{r}
all_flights %>% 
  count(Origin, Dest) %>% 
  arrange(desc(n))
```

What's the relationship between the different delay fields?
```{r}
all_flights %>% 
  filter(!is.na(DepDelay),
         !is.na(LateAircraftDelay)) %>% 
  select(contains("Delay")) %>% 
  head(50)
```

### Airline Comparison
Compare airlines across different metrics
```{r}
(distinct_airlines <- all_flights %>% 
   select(airline) %>% 
   distinct() %>% 
   collect())
```


```{r}
selected_airlines <- c("Southwest Airlines",
                       "Delta Air Lines")
```

```{r}
flights_per_year <- all_flights %>% 
  filter(airline %in% selected_airlines) %>% 
  count(airline, Year, Month) %>% 
  collect()
```

```{r}
flights_per_year %>% 
  ggplot(aes(x = Month, y = n, fill = airline)) +
  facet_wrap(~Year) +
  geom_area() +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(breaks = 1:12) +
  theme(axis.text.x = element_blank()) +
  labs(title = "Flights per Year",
       x = "Month",
       y = "Flights",
       fill = "Airline")
```

## Disconnect
```{r}
spark_disconnect(sc)
```
